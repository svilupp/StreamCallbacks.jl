var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"CurrentModule = StreamCallbacks","category":"page"},{"location":"api/#StreamCallbacks-API-Reference","page":"API","title":"StreamCallbacks API Reference","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"API reference for StreamCallbacks.","category":"page"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/","page":"API","title":"API","text":"Modules = [StreamCallbacks]","category":"page"},{"location":"api/#StreamCallbacks.AbstractStreamCallback","page":"API","title":"StreamCallbacks.AbstractStreamCallback","text":"AbstractStreamCallback\n\nAbstract type for the stream callback.\n\nIt must have the following fields:\n\nout: The output stream, eg, stdout or a pipe.\nflavor: The stream flavor which might or might not differ between different providers, eg, OpenAIStream or AnthropicStream.\nchunks: The list of received AbstractStreamChunk chunks.\nverbose: Whether to print verbose information.\nthrow_on_error: Whether to throw an error if an error message is detected in the streaming response.\nkwargs: Any custom keyword arguments required for your use case.\n\n\n\n\n\n","category":"type"},{"location":"api/#StreamCallbacks.AbstractStreamChunk","page":"API","title":"StreamCallbacks.AbstractStreamChunk","text":"AbstractStreamChunk\n\nAbstract type for the stream chunk.\n\nIt must have the following fields:\n\nevent: The event name.\ndata: The data chunk.\njson: The JSON object or nothing if the chunk does not contain JSON.\n\n\n\n\n\n","category":"type"},{"location":"api/#StreamCallbacks.AbstractStreamFlavor","page":"API","title":"StreamCallbacks.AbstractStreamFlavor","text":"AbstractStreamFlavor\n\nAbstract type for the stream flavor, ie, the API provider.\n\nAvailable flavors:\n\nOpenAIStream for OpenAI API\nAnthropicStream for Anthropic API\n\n\n\n\n\n","category":"type"},{"location":"api/#StreamCallbacks.StreamCallback","page":"API","title":"StreamCallbacks.StreamCallback","text":"StreamCallback\n\nSimplest callback for streaming message, which just prints the content to the output stream defined by out. When streaming is over, it builds the response body from the chunks and returns it as if it was a normal response from the API.\n\nFor more complex use cases, you can define your own callback. See the interface description below for more information.\n\nFields\n\nout: The output stream, eg, stdout or a pipe.\nflavor: The stream flavor which might or might not differ between different providers, eg, OpenAIStream or AnthropicStream.\nchunks: The list of received StreamChunk chunks.\nverbose: Whether to print verbose information. If you enable DEBUG logging, you will see the chunks as they come in.\nthrow_on_error: Whether to throw an error if an error message is detected in the streaming response.\nkwargs: Any custom keyword arguments required for your use case.\n\nInterface\n\nStreamCallback(; kwargs...): Constructor for the StreamCallback object.\nstreamed_request!(cb, url, headers, input): End-to-end wrapper for POST streaming requests.\n\nstreamed_request! composes of:\n\nextract_chunks(flavor, blob): Extract the chunks from the received SSE blob. Returns a list of StreamChunk and the next spillover (if message was incomplete).\ncallback(cb, chunk): Process the chunk to be printed\nextract_content(flavor, chunk): Extract the content from the chunk.\nprint_content(out, text): Print the content to the output stream.\nis_done(flavor, chunk): Check if the stream is done.\nbuild_response_body(flavor, cb): Build the response body from the chunks to mimic receiving a standard response from the API.\n\nIf you want to implement your own callback, you can create your own methods for the interface functions. Eg, if you want to print the streamed chunks into some specialized sink or Channel, you could define a simple method just for print_content.\n\nExample\n\nusing PromptingTools\nconst PT = PromptingTools\n\n# Simplest usage, just provide where to steam the text (we build the callback for you)\nmsg = aigenerate(\"Count from 1 to 100.\"; streamcallback = stdout)\n\nstreamcallback = PT.StreamCallback() # record all chunks\nmsg = aigenerate(\"Count from 1 to 100.\"; streamcallback)\n# this allows you to inspect each chunk with `streamcallback.chunks`\n\n# Get verbose output with details of each chunk for debugging\nstreamcallback = PT.StreamCallback(; verbose=true, throw_on_error=true)\nmsg = aigenerate(\"Count from 1 to 10.\"; streamcallback)\n\nNote: If you provide a StreamCallback object to aigenerate, we will configure it and necessary api_kwargs via configure_callback! unless you specify the flavor field. If you provide a StreamCallback with a specific flavor, we leave all configuration to the user (eg, you need to provide the correct api_kwargs).\n\n\n\n\n\n","category":"type"},{"location":"api/#StreamCallbacks.StreamChunk","page":"API","title":"StreamCallbacks.StreamChunk","text":"StreamChunk\n\nA chunk of streaming data. A message is composed of multiple chunks.\n\nFields\n\nevent: The event name.\ndata: The data chunk.\njson: The JSON object or nothing if the chunk does not contain JSON.\n\n\n\n\n\n","category":"type"},{"location":"api/#StreamCallbacks.build_response_body-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamCallback}","page":"API","title":"StreamCallbacks.build_response_body","text":"build_response_body(\n    flavor::AnthropicStream, cb::AbstractStreamCallback; verbose::Bool = false, kwargs...)\n\nBuild the response body from the chunks to mimic receiving a standard response from the API.\n\nNote: Limited functionality for now. Does NOT support tool use. Use standard responses for these.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.build_response_body-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamCallback}","page":"API","title":"StreamCallbacks.build_response_body","text":"build_response_body(flavor::OpenAIStream, cb::AbstractStreamCallback; verbose::Bool = false, kwargs...)\n\nBuild the response body from the chunks to mimic receiving a standard response from the API.\n\nNote: Limited functionality for now. Does NOT support tool use, refusals, logprobs. Use standard responses for these.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.callback-Tuple{StreamCallbacks.AbstractStreamCallback, StreamCallbacks.AbstractStreamChunk}","page":"API","title":"StreamCallbacks.callback","text":"callback(cb::AbstractStreamCallback, chunk::AbstractStreamChunk; kwargs...)\n\nProcess the chunk to be printed and print it. It's a wrapper for two operations:\n\nextract the content from the chunk using extract_content\nprint the content to the output stream using print_content\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.extract_chunks-Tuple{StreamCallbacks.AbstractStreamFlavor, AbstractString}","page":"API","title":"StreamCallbacks.extract_chunks","text":"extract_chunks(flavor::AbstractStreamFlavor, blob::AbstractString;\n    spillover::AbstractString = \"\", verbose::Bool = false, kwargs...)\n\nExtract the chunks from the received SSE blob. Shared by all streaming flavors currently.\n\nReturns a list of StreamChunk and the next spillover (if message was incomplete).\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.extract_content-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamChunk}","page":"API","title":"StreamCallbacks.extract_content","text":"extract_content(flavor::AnthropicStream, chunk::AbstractStreamChunk; kwargs...)\n\nExtract the content from the chunk.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.extract_content-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"API","title":"StreamCallbacks.extract_content","text":"extract_content(flavor::OpenAIStream, chunk::AbstractStreamChunk; kwargs...)\n\nExtract the content from the chunk.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.handle_error_message-Tuple{StreamCallbacks.AbstractStreamChunk}","page":"API","title":"StreamCallbacks.handle_error_message","text":"handle_error_message(chunk::AbstractStreamChunk; throw_on_error::Bool = false, kwargs...)\n\nHandles error messages from the streaming response.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.is_done-Tuple{OllamaStream, StreamChunk}","page":"API","title":"StreamCallbacks.is_done","text":"Terminates the stream when the done key in the JSON object is true.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.is_done-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"API","title":"StreamCallbacks.is_done","text":"is_done(flavor::OpenAIStream, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if the streaming is done. Shared by all streaming flavors currently.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.print_content-Tuple{Channel, AbstractString}","page":"API","title":"StreamCallbacks.print_content","text":"print_content(out::Channel, text::AbstractString; kwargs...)\n\nPrint the content to the provided Channel out.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.print_content-Tuple{IO, AbstractString}","page":"API","title":"StreamCallbacks.print_content","text":"print_content(out::IO, text::AbstractString; kwargs...)\n\nPrint the content to the IO output stream out.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.print_content-Tuple{Nothing, Any}","page":"API","title":"StreamCallbacks.print_content","text":"print_content(out::Nothing, text::Any)\n\nDo nothing if the output stream is nothing.\n\n\n\n\n\n","category":"method"},{"location":"api/#StreamCallbacks.streamed_request!-Tuple{StreamCallbacks.AbstractStreamCallback, Any, Any, Any}","page":"API","title":"StreamCallbacks.streamed_request!","text":"streamed_request!(cb::AbstractStreamCallback, url, headers, input; kwargs...)\n\nEnd-to-end wrapper for POST streaming requests.  In-place modification of the callback object (cb.chunks) with the results of the request being returned. We build the body of the response object in the end and write it into the resp.body.\n\nReturns the response object.\n\nArguments\n\ncb: The callback object.\nurl: The URL to send the request to.\nheaders: The headers to send with the request.\ninput: A buffer with the request body.\nkwargs: Additional keyword arguments.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = StreamCallbacks","category":"page"},{"location":"#StreamCallbacks.jl-Documentation","page":"Home","title":"StreamCallbacks.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StreamCallbacks.jl is designed to unify streaming interfaces for Large Language Models (LLMs) across multiple providers. It simplifies handling Server-Sent Events (SSE), provides easy debugging by collecting all chunks, and offers various built-in sinks (e.g., stdout, channels, pipes) for streaming data. You can also extend it to implement custom logic for processing streamed data.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Unified Streaming Interface: Provides a consistent API for streaming responses from various LLM providers.\nEasy Debugging: Collects all received chunks, enabling detailed inspection and debugging.\nBuilt-in Sinks: Supports common sinks like stdout, channels, and pipes out of the box.\nCustomizable Callbacks: Extendable interface allows you to define custom behavior for each received chunk.","category":"page"},{"location":"#Supported-Providers","page":"Home","title":"Supported Providers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OpenAI API (and all compatible providers)\nAnthropic API","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install StreamCallbacks.jl via the package manager (it's not registered yet):","category":"page"},{"location":"","page":"Home","title":"Home","text":"import Pkg\nPkg.add(path=\"https://github.com/svilupp/StreamCallbacks.jl\")","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StreamCallbacks.jl revolves around the StreamCallback type, which manages the streaming of messages and the handling of received chunks. Here's a simple example of how to use it:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StreamCallbacks\n\n# Create a StreamCallback object that streams output to stdout\ncb = StreamCallback(out = stdout)\n\n# Use the callback with your API request (see Usage Examples below)","category":"page"},{"location":"#Usage-Examples","page":"Home","title":"Usage Examples","text":"","category":"section"},{"location":"#Example-with-OpenAI-API","page":"Home","title":"Example with OpenAI API","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using HTTP\nusing JSON3\nusing StreamCallbacks\n\n# Prepare target URL and headers\nurl = \"https://api.openai.com/v1/chat/completions\"\nheaders = [\n    \"Content-Type\" => \"application/json\",\n    \"Authorization\" => \"Bearer $(get(ENV, \"OPENAI_API_KEY\", \"\"))\"\n]\n\n# Create a StreamCallback object\ncb = StreamCallback(out = stdout, flavor = OpenAIStream())\n\n# Prepare the request payload\nmessages = [Dict(\"role\" => \"user\", \"content\" => \"Count from 1 to 100.\")]\npayload = IOBuffer()\nJSON3.write(payload, (; stream = true, messages, model = \"gpt-4o-mini\", stream_options = (; include_usage = true)))\n\n# Send the streamed request\nresp = streamed_request!(cb, url, headers, payload)\n\n# Check the response\nprintln(\"Response status: \", resp.status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: For debugging, you can set verbose = true in the StreamCallback constructor to get detailed logs of each chunk. Ensure you enable DEBUG logging level in your environment.","category":"page"},{"location":"#Example-with-PromptingTools.jl","page":"Home","title":"Example with PromptingTools.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StreamCallbacks.jl is integrated with PromptingTools.jl, allowing you to easily handle streaming in AI generation tasks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using PromptingTools\nconst PT = PromptingTools\n\n# Simplest usage: stream output to stdout (the callback is built for you)\nmsg = aigenerate(\"Count from 1 to 100.\"; streamcallback = stdout)\n\n# Create a StreamCallback object to record all chunks\nstreamcallback = PT.StreamCallback()\nmsg = aigenerate(\"Count from 1 to 100.\"; streamcallback)\n# You can inspect each chunk with `streamcallback.chunks`\n\n# Get verbose output with details of each chunk for debugging\nstreamcallback = PT.StreamCallback(verbose = true, throw_on_error = true)\nmsg = aigenerate(\"Count from 1 to 10.\"; streamcallback)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: If you provide a StreamCallback object to aigenerate, PromptingTools.jl will configure it and necessary api_kwargs via configure_callback! unless you specify the flavor field. If you provide a StreamCallback with a specific flavor, you need to provide the correct api_kwargs yourself.","category":"page"},{"location":"#Extending-StreamCallbacks.jl","page":"Home","title":"Extending StreamCallbacks.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For more complex use cases, you can define your own callback methods. This allows you to customize how each chunk is processed. Here's how the interface works:","category":"page"},{"location":"#StreamCallback-Interface","page":"Home","title":"StreamCallback Interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Constructor: StreamCallback(; kwargs...) creates a new StreamCallback object.\nstreamed_request!: streamed_request!(cb, url, headers, input) sends a streaming POST request and processes the response using the callback.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The streamed_request! function internally calls:","category":"page"},{"location":"","page":"Home","title":"Home","text":"extract_chunks: extract_chunks(flavor, blob) extracts chunks from the received SSE blob.\ncallback: callback(cb, chunk) processes each received chunk.\nextract_content: extract_content(flavor, chunk) extracts the content from the chunk.\nprint_content: print_content(out, text) prints the content to the output stream.\nis_done: is_done(flavor, chunk) checks if the streaming is complete.\nbuild_response_body: build_response_body(flavor, cb) builds the final response body from the collected chunks.","category":"page"},{"location":"#Custom-Callback-Example","page":"Home","title":"Custom Callback Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Suppose you want to process each chunk and send it to a custom sink, such as a logging system or a GUI component. You can extend the print_content method:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StreamCallbacks\n\nstruct MyCustomCallback <: StreamCallbacks.AbstractCallback\n    out::IO\n    # ... add additional fields if necessary\nend\n\nfunction StreamCallbacks.callback(cb::MyCustomCallback, chunk::StreamChunk; kwargs...)\n    # Custom logic to handle the text\n    println(\"Received chunk: \", chunk.data)\n    # For example, send the text to a GUI component or log it\nend","category":"page"}]
}
